import os
from dotenv import load_dotenv
import gradio as gr
from pathlib import Path
from typing import List, Dict, Any
from nano_graphrag import QueryParam
from base import rag_instance
# from llm_service import set_usage_file, gemini_model_if_cache, gpt_4o_complete, gpt_4o_mini_complete, gemini_complete, gemini_stream
from nano_graphrag.prompt import PROMPTS
from google import genai
from google.genai import types
from openai import AsyncOpenAI
WORKING_DIR="./health_care"
rag = rag_instance(WORKING_DIR)
use_model_func=rag.best_model_func
gr.set_static_paths(paths=[Path.cwd() / "assets"])

load_dotenv(dotenv_path="../.env")
api_key = os.getenv("API_KEY")
GPT_KEY=os.getenv("GPT_KEY")
GPT_VIP_KEY=os.getenv("GPT_VIP_KEY")
GEMINI_KEY=os.getenv("GEMINI_KEY")
GEMINI_API1 = os.getenv("GEMINI_API1")
GEMINI_API2 = os.getenv("GEMINI_API2")
GEMINI_API3 = os.getenv("GEMINI_API3")
GEMINI_API4 = os.getenv("GEMINI_API4")
GEMINI_API5 = os.getenv("GEMINI_API5")
GEMINI_API6 = os.getenv("GEMINI_API6")
GEMINI_API7 = os.getenv("GEMINI_API7")
GEMINI_API8 = os.getenv("GEMINI_API8")
GEMINI_API9 = os.getenv("GEMINI_API9")
GEMINI_API10 = os.getenv("GEMINI_API10")
GEMINI_API11 = os.getenv("GEMINI_API11")
GEMINI_API12 = os.getenv("GEMINI_API12")

async def navigator(
    user_input: str,
    history: list,):
    prompt = """B·∫°n l√† m·ªôt chuy√™n gia ƒëi·ªÅu h∆∞·ªõng th√¥ng tin trong m·ªôt h·ªá th·ªëng t∆∞ v·∫•n s·ª©c kh·ªèe tim m·∫°ch. Nhi·ªám v·ª• c·ªßa b·∫°n l√† ph√¢n t√≠ch c√¢u h·ªèi m·ªõi nh·∫•t c·ªßa ng∆∞·ªùi d√πng, k·∫øt h·ª£p v·ªõi ng·ªØ c·∫£nh cu·ªôc h·ªôi tho·∫°i tr∆∞·ªõc ƒë√≥ (n·∫øu c√≥), v√† quy·∫øt ƒë·ªãnh h√†nh ƒë·ªông ph√π h·ª£p nh·∫•t. H√£y l√†m theo h∆∞·ªõng d·∫´n d∆∞·ªõi ƒë√¢y v√† ch·ªâ ƒë∆∞a ra m·ªôt lo·∫°i output:

1. N·∫øu c√¢u h·ªèi hi·ªán t·∫°i **kh√¥ng li√™n quan ƒë·∫øn n·ªôi dung cu·ªôc tr√≤ chuy·ªán tr∆∞·ªõc ƒë√≥** ho·∫∑c **kh√¥ng li√™n quan ƒë·∫øn lƒ©nh v·ª±c s·ª©c kh·ªèe tim m·∫°ch**, h√£y Output: **"tr∆∞·ªùng h·ª£p 1"**.

2. N·∫øu c√¢u h·ªèi hi·ªán t·∫°i **li√™n quan ƒë·∫øn lƒ©nh v·ª±c tim m·∫°ch** nh∆∞ng **c·∫ßn th√™m th√¥ng tin t·ª´ h·ªá th·ªëng c∆° s·ªü tri th·ª©c**, h√£y:
   - Output: **"tr∆∞·ªùng h·ª£p 2"**
   - Vi·∫øt l·∫°i c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng sao cho r√µ r√†ng, ƒë·∫ßy ƒë·ªß, lo·∫°i b·ªè m∆° h·ªì(tri·ªáu ch·ª©ng n√†y, b·ªánh ƒë√≥, n√≥,...).

---
V√≠ d·ª•:
L·ªãch s·ª≠ h·ªôi tho·∫°i:
user: T√¥i b·ªã ƒëau th·∫Øt ng·ª±c khi g·∫Øng s·ª©c, c√≥ th·ªÉ l√† do ƒë√¢u?
assistant: Tri·ªáu ch·ª©ng ƒëau th·∫Øt ng·ª±c khi g·∫Øng s·ª©c c√≥ th·ªÉ l√† d·∫•u hi·ªáu c·ªßa b·ªánh m·∫°ch v√†nh, m·ªôt b·ªánh l√Ω tim m·∫°ch nghi√™m tr·ªçng. B·∫°n n√™n ƒëi kh√°m b√°c sƒ© ƒë·ªÉ ƒë∆∞·ª£c ch·∫©n ƒëo√°n ch√≠nh x√°c.
user: B·ªánh m·∫°ch v√†nh l√† g√¨?
assistant: B·ªánh m·∫°ch v√†nh l√† t√¨nh tr·∫°ng c√°c m·∫°ch m√°u cung c·∫•p m√°u cho tim b·ªã h·∫πp ho·∫∑c t·∫Øc ngh·∫Ωn, th∆∞·ªùng do m·∫£ng x∆° v·ªØa. ƒêi·ªÅu n√†y c√≥ th·ªÉ d·∫´n ƒë·∫øn nh·ªìi m√°u c∆° tim n·∫øu kh√¥ng ƒë∆∞·ª£c ƒëi·ªÅu tr·ªã k·ªãp th·ªùi.
V√≠ d·ª• 1: 
T√¥i n√™n du l·ªãch ·ªü ƒë√¢u v√†o m√πa thu n√†y?
Output: tr∆∞·ªùng h·ª£p 1
V√≠ d·ª• 2:
B·ªánh l√Ω tr√™n c√≥ nguy hi·ªÉm kh√¥ng?
Output: tr∆∞·ªùng h·ª£p 2
B·ªánh m·∫°ch v√†nh c√≥ nguy hi·ªÉm kh√¥ng?

---
D·ªØ li·ªáu th·∫≠t:
C√¢u h·ªèi : {user_input}

H√£y ƒë∆∞a ra **duy nh·∫•t m·ªôt output** theo ƒë·ªãnh d·∫°ng sau:
- N·∫øu l√† tr∆∞·ªùng h·ª£p 1: ch·ªâ c·∫ßn in ƒë√∫ng `"tr∆∞·ªùng h·ª£p 1"`
- N·∫øu l√† tr∆∞·ªùng h·ª£p 2: in `"tr∆∞·ªùng h·ª£p 2"` r·ªìi xu·ªëng d√≤ng v√† vi·∫øt l·∫°i c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch r√µ r√†ng

"""
    message , history_full = history_message(prompt.format(user_input=user_input), history)
    messages =  format_message_history(history_full)
    global_openai_async_client = AsyncOpenAI(api_key=GEMINI_API10,
                                                base_url="https://generativelanguage.googleapis.com/v1beta/openai/")
    response = await global_openai_async_client.chat.completions.create(
        model="gemini-2.0-flash",
        messages=messages
        ) 
    
    output = response.choices[0].message.content.strip()

    if output == "tr∆∞·ªùng h·ª£p 1":
        return "tr∆∞·ªùng h·ª£p 1",""

    elif output.startswith("tr∆∞·ªùng h·ª£p 2"):
        # T√°ch d√≤ng th·ª© 2 l√† c√¢u h·ªèi ƒë√£ vi·∫øt l·∫°i
        lines = output.split("\n")
        if len(lines) > 1:
            # rewritten_question = lines[1].strip()
            rewritten_question = "\n".join(line.strip() for line in lines[1:])
            return "tr∆∞·ªùng h·ª£p 2",rewritten_question
        else:
            return "L·ªói: Kh√¥ng t√¨m th·∫•y c√¢u h·ªèi vi·∫øt l·∫°i trong tr∆∞·ªùng h·ª£p 2",""

    else:
        return "L·ªói: Kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c lo·∫°i output t·ª´ h·ªá th·ªëng",""
    
def history_message(
    user_input: str,
    history: list,
) -> tuple:
    # Logging 
    history_full = history.copy()  # Create a copy of the history for logging
    history_full.append({
        "role": "user",
        "content": user_input,
    })
    return "", history_full

def format_message_history(
    history: list,
) -> List[Dict]:
    formatted_messages = []

    # Add previous messages
    for msg in history:
        if msg["role"] == "user":
            formatted_messages.append({
                "role": "user",
                "content": msg["content"]
            })
        elif msg["role"] == "assistant":
            if "metadata" not in msg or msg["metadata"] is None:
                formatted_messages.append({
                    "role": "assistant",
                    "content": msg["content"]
                })
    return formatted_messages

async def bot_response(
    user_input: str,
    history: list,
):
    try:
        navi, rewrite = await navigator(user_input, history)
        print(f"Navigator output: {navi}, Rewritten question: {rewrite}")
        if navi == "tr∆∞·ªùng h·ª£p 1":
            # If the question is not related to the context or health care, return a default response
            yield {
                "role": "assistant",
                "content": "Xin l·ªói, c√¢u h·ªèi c·ªßa b·∫°n n·∫±m ngo√†i ph·∫°m vi t∆∞ v·∫•n s·ª©c kh·ªèe tim m·∫°ch."
                }
        elif navi == "tr∆∞·ªùng h·ª£p 2":
            context, chunk_list =await rag.aquery(
                rewrite,
                param=QueryParam(
                    # mode="my_query",local
                    mode="my_query_context",
                    top_k_triples= 5,
                    top_k_chunks=10,
                    chunk_weight = 0.19,
                    damping_factor= 0.6,
                    num_context_chunks= 10,
                    k_hops = 3,
                    k_paths = 3
                )
            )
            # Initialize main response and citations
            citations = []
            for chunk in chunk_list:
                citations.append(chunk)

            system_prompt = PROMPTS["my_query_system"]
            prompt = PROMPTS["my_query"].format(
                context_data=context,
                question=rewrite
            )
            message , history_full = history_message(prompt, history)
            messages = [{"role": "system", "content": system_prompt}] + format_message_history(history_full)

            global_openai_async_client = AsyncOpenAI(api_key=GEMINI_API11,
                                                base_url="https://generativelanguage.googleapis.com/v1beta/openai/")
            response = await global_openai_async_client.chat.completions.create(
                model="gemini-2.0-flash",
                messages=messages,
                stream=True
                ) 
            main_response = ""
            async for chunk in response:
                delta = chunk.choices[0].delta
                if getattr(delta, "content", None):
                    main_response += delta.content
                yield {
                    "role": "assistant",
                    "content": main_response
                }

            yield [
                {
                "role": "assistant",
                "content": main_response
                },
                {
                "role": "assistant",
                "content": "\n".join([f"# ‚Ä¢ VƒÉn b·∫£n {stt}:\n{cite}" for stt, cite in enumerate(citations, start=1)]),
                "metadata": {"title": "üìö Ngu·ªìn tham kh·∫£o", "status": "done"},
                }]

    except Exception as e:
        print(f"Error in bot_response: {str(e)}")
        error_message = str(e)
        history.append({
            "role": "assistant",
            "content": f"I apologize, but I encountered an error: {error_message}"
        })
        yield history

css = """
  .logo-text { display: flex; align-items: center; gap: 10px; margin-bottom: 20px; }
  .logo-text img { max-height: 60px; }
  .logo-text .chat-name { font-size: 2em; font-weight: bold; }
"""
with gr.Blocks(css=css, theme=gr.themes.Default(primary_hue=gr.themes.colors.red), fill_height=True) as demo:
    gr.HTML("""
      <div class="logo-text">
        <img src="/gradio_api/file=assets/heart.png" alt="Logo"/>
        <span class="chat-name">S·ª©c Kh·ªèe Tim M·∫°ch</span>
      </div>
    """)
    gr.ChatInterface(
        fn=bot_response,
        type="messages",
        description="Gi·∫£i ƒë√°p c√¢u h·ªèi v·ªÅ s·ª©c kh·ªèe tim m·∫°ch. B·∫°n c√≥ th·ªÉ h·ªèi v·ªÅ c√°c tri·ªáu ch·ª©ng, b·ªánh l√Ω, ph∆∞∆°ng ph√°p ƒëi·ªÅu tr·ªã v√† nhi·ªÅu v·∫•n ƒë·ªÅ kh√°c li√™n quan ƒë·∫øn s·ª©c kh·ªèe tim m·∫°ch.",
        save_history=True,
        autoscroll=True,
        # examples=[
        #     ["What is the color of the grass?"],
        #     ["Tell me about the sky."],
        #     ["What is the sun's color?"]
        # ]
    )


if __name__ == "__main__":
    demo.launch(debug=True, allowed_paths=["assets"])
